{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = 'llama3.2'\n",
    "TRANSFORMER = 'all-MiniLM-L6-v2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "model = Ollama(model=MODEL)\n",
    "embeddings = OllamaEmbeddings(model=MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader('dataset/book.pdf')\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean and preprocess the text:\n",
    "    - Remove unnecessary newlines\n",
    "    - Fix broken words at line breaks\n",
    "    \"\"\"\n",
    "    text = re.sub(r'-\\n', '', text) \n",
    "    text = re.sub(r'\\n+', ' ', text) \n",
    "    text = re.sub(r'\\s+', ' ', text) \n",
    "    return text.strip()\n",
    "\n",
    "def format_pages(pages):\n",
    "    \"\"\"Convert the `pages` list into a structured list of dictionaries with cleaned text.\"\"\"\n",
    "    formatted_data = []\n",
    "    for page in pages:\n",
    "        page_number = page.metadata.get('page', 0)\n",
    "        text = page.page_content\n",
    "        cleaned_text = clean_text(text)\n",
    "        formatted_data.append({\"page\": page_number, \"text\": cleaned_text})\n",
    "    return formatted_data\n",
    "\n",
    "book_data = format_pages(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "dataset = Dataset.from_list(book_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 28/28 [00:07<00:00,  3.83it/s]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "\n",
    "embedding_model = SentenceTransformer(TRANSFORMER)\n",
    "embeddings = embedding_model.encode([item['text'] for item in book_data], show_progress_bar=True)\n",
    "dimension = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_metadata = {i: item for i, item in enumerate(book_data)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_sections(query, k=3):\n",
    "    \"\"\"Retrieve the top-k relevant sections based on the query.\"\"\"\n",
    "    \n",
    "    query_embedding = embedding_model.encode([query])\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "    results = [{\"distance\": d, \"page\": page_metadata[i]['page'], \"text\": page_metadata[i]['text']} for d, i in zip(distances[0], indices[0])]\n",
    "    \n",
    "    pages = [result[\"page\"] for result in results]\n",
    "    distances = [result[\"distance\"] for result in results]\n",
    "    context = \" \".join([result[\"text\"] for result in results])\n",
    "    final_results = {\"pages\": pages, \"distances\": distances, \"context\": context}\n",
    "    \n",
    "    return final_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "Answer the question based on the context below. If you can't answer the question, reply \"I don't know\". \n",
    "Answer in complete sentences and in paragraph form. Donot break paragraph and donot use newline character.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | model | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "submission_results = []\n",
    "\n",
    "with open('dataset/queries.json') as f:\n",
    "    queries = json.load(f)\n",
    "    for query in queries:\n",
    "        query_id = query['query_id'] \n",
    "        question = query['question']\n",
    "        \n",
    "        results = retrieve_sections(question)\n",
    "        \n",
    "        context = results['context']\n",
    "        pages = results['pages']\n",
    "        distance = results['distances']\n",
    "        \n",
    "        answer = chain.invoke({'context': context, 'question': question})\n",
    "        \n",
    "        submission_results.append({\n",
    "            'ID': query_id,\n",
    "            'question': question,\n",
    "            'answer': answer,\n",
    "            'context': context,\n",
    "            'references': \"{'pages': '\" + str(pages) + \"'}\"\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "\n",
    "def save_json_to_csv(json_data, csv_filename):\n",
    "    def clean_text(text):\n",
    "        return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    cleaned_data = [\n",
    "        {key: clean_text(value) if key == 'answer' else value for key, value in entry.items() if key != 'question'}\n",
    "        for entry in json_data\n",
    "    ]\n",
    "\n",
    "    with open(csv_filename, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=['ID', 'context', 'answer', 'references'])\n",
    "        writer.writeheader()\n",
    "        writer.writerows(cleaned_data)\n",
    "\n",
    "save_json_to_csv(submission_results, 'submission.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
